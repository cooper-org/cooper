{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a logistic regression classifier on MNIST under a norm constraint.\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cooper-org/cooper/blob/main/docs/source/notebooks/plot_mnist_logistic_regression.ipynb)\n",
    "\n",
    "\n",
    ":::{note}\n",
    "This example illustrates how to use **Cooper** on a simple machine learning problem\n",
    "that involves using mini-batches of data.\n",
    ":::\n",
    "\n",
    "In this example, we consider a simple convex constrained optimization problem: training\n",
    "a Logistic Regression classifier on the MNIST dataset. The model is constrained so that\n",
    "the squared L2 norm of its parameters is less than 1.\n",
    "\n",
    "Although we employ a simple Logistic Regression model, the same principles can be applied\n",
    "\n",
    "This example illustrates how **Cooper** integrates with a typical PyTorch training\n",
    "pipeline, where:\n",
    "- models are defined using a ``torch.nn.Module``,\n",
    "- steps loop over mini-batches of data,\n",
    "- CUDA acceleration is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install cooper-optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import cooper\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class NormConstrainedLogisticRegression(cooper.ConstrainedMinimizationProblem):\n",
    "    def __init__(self, constraint_level: float = 1.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.constraint_level = constraint_level\n",
    "\n",
    "        # The multiplier must be on the same device as the model\n",
    "        multiplier = cooper.multipliers.DenseMultiplier(num_constraints=1, device=DEVICE)\n",
    "\n",
    "        self.norm_constraint = cooper.Constraint(\n",
    "            constraint_type=cooper.ConstraintType.INEQUALITY,\n",
    "            formulation_type=cooper.formulations.Lagrangian,\n",
    "            multiplier=multiplier,\n",
    "        )\n",
    "\n",
    "    def compute_cmp_state(self, model: torch.nn.Module, inputs: torch.Tensor, targets: torch.Tensor) -> cooper.CMPState:\n",
    "        logits = model(inputs.view(inputs.shape[0], -1))\n",
    "        loss = torch.nn.functional.cross_entropy(logits, targets)\n",
    "        accuracy = (logits.argmax(dim=1) == targets).float().mean()\n",
    "\n",
    "        sq_l2_norm = model.weight.pow(2).sum() + model.bias.pow(2).sum()\n",
    "\n",
    "        # Constraint violations use convention g <= 0\n",
    "        constraint_state = cooper.ConstraintState(violation=sq_l2_norm - self.constraint_level)\n",
    "\n",
    "        # Create a CMPState object, which contains the loss and observed constraints\n",
    "        observed_constraints = {self.norm_constraint: constraint_state}\n",
    "        return cooper.CMPState(loss=loss, observed_constraints=observed_constraints, misc={\"accuracy\": accuracy})\n",
    "\n",
    "\n",
    "# Load the MNIST dataset\n",
    "data_path = \"./data\"\n",
    "data_transforms = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "dataset = datasets.MNIST(data_path, train=True, download=True, transform=data_transforms)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=128, num_workers=2, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "model = torch.nn.Linear(in_features=28 * 28, out_features=10, bias=True)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Instantiate the constrained optimization problem\n",
    "cmp = NormConstrainedLogisticRegression(constraint_level=1.0)\n",
    "\n",
    "# Instantiate a PyTorch optimizer for the primal variables\n",
    "primal_optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, amsgrad=True)\n",
    "\n",
    "# Instantiate PyTorch optimizer for the dual variables\n",
    "dual_optimizer = torch.optim.SGD(cmp.dual_parameters(), lr=1e-3, maximize=True)\n",
    "\n",
    "# Instantiate the Cooper optimizer\n",
    "cooper_optimizer = cooper.optim.SimultaneousOptimizer(\n",
    "    primal_optimizers=primal_optimizer, dual_optimizers=dual_optimizer, cmp=cmp\n",
    ")\n",
    "\n",
    "# Create a directory to save checkpoints\n",
    "checkpoint_path = \"./checkpoint\"\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "\n",
    "# Load checkpoint if exists\n",
    "if not os.path.isfile(checkpoint_path + \"/checkpoint.pth\"):\n",
    "    batch_ix = 0\n",
    "    start_epoch = 0\n",
    "    all_metrics = defaultdict(list)\n",
    "else:\n",
    "    checkpoint = torch.load(checkpoint_path + \"/checkpoint.pth\", weights_only=True)\n",
    "    batch_ix = checkpoint[\"batch_ix\"]\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "    all_metrics = checkpoint[\"all_metrics\"]\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    cmp.load_state_dict(checkpoint[\"cmp_state_dict\"])\n",
    "    cooper_optimizer.load_state_dict(checkpoint[\"cooper_optimizer_state_dict\"])\n",
    "\n",
    "for epoch_num in range(start_epoch, 7):\n",
    "    for inputs, targets in train_loader:\n",
    "        batch_ix += 1\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            inputs, targets = inputs.cuda(non_blocking=True), targets.cuda(non_blocking=True)\n",
    "\n",
    "        _, cmp_state, primal_lagrangian_store, _ = cooper_optimizer.roll(\n",
    "            compute_cmp_state_kwargs={\"model\": model, \"inputs\": inputs, \"targets\": targets}\n",
    "        )\n",
    "\n",
    "        if batch_ix % 3 == 0:\n",
    "            all_metrics[\"batch_ix\"].append(batch_ix)\n",
    "            all_metrics[\"train_loss\"].append(cmp_state.loss.item())\n",
    "            all_metrics[\"train_acc\"].append(cmp_state.misc[\"accuracy\"].item())\n",
    "\n",
    "            multiplier_value = primal_lagrangian_store.multiplier_values[cmp.norm_constraint].item()\n",
    "            all_metrics[\"multiplier_value\"].append(multiplier_value)\n",
    "\n",
    "            constraint_violation = cmp_state.observed_constraints[cmp.norm_constraint].violation\n",
    "            all_metrics[\"constraint_violation\"].append(constraint_violation.item())\n",
    "\n",
    "    # Save checkpoint at the end of each epoch\n",
    "    torch.save(\n",
    "        {\n",
    "            \"batch_ix\": batch_ix,\n",
    "            \"epoch\": epoch_num,\n",
    "            \"all_metrics\": all_metrics,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"cmp_state_dict\": cmp.state_dict(),\n",
    "            \"cooper_optimizer_state_dict\": cooper_optimizer.state_dict(),\n",
    "        },\n",
    "        checkpoint_path + \"/checkpoint.pth\",\n",
    "    )\n",
    "\n",
    "del batch_ix, all_metrics, model, cmp, cooper_optimizer\n",
    "\n",
    "# Post-training analysis and plotting\n",
    "all_metrics = torch.load(checkpoint_path + \"/checkpoint.pth\", weights_only=True)[\"all_metrics\"]\n",
    "\n",
    "fig, (ax0, ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=4, sharex=True, figsize=(18, 4))\n",
    "\n",
    "ax0.plot(all_metrics[\"batch_ix\"], all_metrics[\"train_loss\"])\n",
    "ax0.set_xlabel(\"Batch\")\n",
    "ax0.set_title(\"Training Loss\")\n",
    "\n",
    "ax1.plot(all_metrics[\"batch_ix\"], all_metrics[\"train_acc\"])\n",
    "ax1.set_xlabel(\"Batch\")\n",
    "ax1.set_title(\"Training Acc\")\n",
    "\n",
    "ax2.plot(all_metrics[\"batch_ix\"], np.stack(all_metrics[\"multiplier_value\"]))\n",
    "ax2.set_xlabel(\"Batch\")\n",
    "ax2.set_title(\"Inequality Multiplier\")\n",
    "\n",
    "ax3.plot(all_metrics[\"batch_ix\"], np.stack(all_metrics[\"constraint_violation\"]))\n",
    "# Show that defect converges close to zero\n",
    "ax3.axhline(0.0, c=\"gray\", alpha=0.5)\n",
    "ax3.set_xlabel(\"Batch\")\n",
    "ax3.set_title(\"Inequality Defect\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
