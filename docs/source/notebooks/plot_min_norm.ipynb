{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the min-norm solution to a linear system of equations.\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cooper-org/cooper/blob/main/docs/source/notebooks/plot_min_norm.ipynb)\n",
    "\n",
    "\n",
    "This example considers the problem of finding the min-L2-norm solution to a system of\n",
    "linear equations. The problem is formulated as a constrained minimization problem:\n",
    "\n",
    "$$\n",
    "\\min_{x}  \\,\\, \\Vert x \\Vert_2^2  \\,\\, s.t. \\,\\, Ax = b\n",
    "$$\n",
    "\n",
    "where $A$ is a matrix of size $(m, n)$ and $b$ is a vector of size\n",
    "$(m, 1)$.\n",
    "\n",
    "This is a well-known convex problem in numerical linear algebra, whose solution is given\n",
    "by the vector $x^*$ satisfying $A^{\\dagger}b = x^*$, where\n",
    "$A^{\\dagger}$ is the Moore-Penrose pseudo-inverse of $A$.\n",
    "\n",
    "Here we analyze this traditional problem under the framework of gradient-based\n",
    "Lagrangian optimization. We allow for the possibility that the system of equations\n",
    "is partially observed at each iteration. That is, we assume that the matrix $A$\n",
    "and the vector $b$ may be sub-sampled at each iteration.\n",
    "\n",
    "$$\n",
    "\\Lag(x, \\lambda) = \\Vert x \\Vert_2^2 + \\lambda^T D (Ax - b)\n",
    "$$\n",
    "\n",
    "where $\\lambda$ is the vector of Lagrange multipliers, and $D$ is a\n",
    "stochastic diagonal matrix with $1$ on the indices corresponding to the observed\n",
    "equations and $0$ everywhere else.\n",
    "\n",
    "The results below illustrate the influence of the number of observed equations on the\n",
    "convergence of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install cooper-optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import BatchSampler, RandomSampler\n",
    "\n",
    "import cooper\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def create_linear_system(num_equations: int, num_variable: int, seed: int = 0):\n",
    "    \"\"\"Create a solvable linear system with a well-behaved matrix spectrum.\n",
    "\n",
    "    Args:\n",
    "        num_equations: Number of equations in the linear system.\n",
    "        num_variable: Number of variables in the linear system.\n",
    "        seed: The seed value to set for random number generation.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed=seed)\n",
    "\n",
    "    # Create a random linear system with 1-singular values\n",
    "    U, _, V = torch.linalg.svd(torch.randn(num_equations, num_variable))\n",
    "    S = torch.eye(num_equations, num_variable)\n",
    "    A = U @ S @ V.T\n",
    "\n",
    "    # Build a solvable linear system\n",
    "    b = A @ torch.randn(num_variable, 1)\n",
    "    b /= torch.linalg.vector_norm(b)\n",
    "\n",
    "    # Build min-norm solution based on the SVD of A\n",
    "    x_optim = V @ (S.T @ (U.T @ b))\n",
    "\n",
    "    # Verify that the analytical solution is sufficiently close numerically\n",
    "    assert torch.allclose(A @ x_optim, b, atol=1e-5)\n",
    "\n",
    "    A, b, x_optim = A.to(DEVICE), b.to(DEVICE), x_optim.to(DEVICE)\n",
    "\n",
    "    return A, b, x_optim\n",
    "\n",
    "\n",
    "class LinearConstraintDataset(Dataset):\n",
    "    \"\"\"Dataset comprising equations in a linear constraint system. This dataset is used\n",
    "    to sample equations for the equality constraints in the min-norm problem.\n",
    "\n",
    "    Args:\n",
    "        A: Matrix of the linear system.\n",
    "        b: Right-hand side of the linear system.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, A: torch.Tensor, b: torch.Tensor):\n",
    "        self.A, self.b = A, b\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.A.shape[0]\n",
    "\n",
    "    def __getitem__(self, index: list[int]) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        index = torch.tensor(index)\n",
    "        return self.A[index], self.b[index], index\n",
    "\n",
    "\n",
    "def instantiate_dataloader(dataset: Dataset, batch_size: int, seed=0) -> DataLoader:\n",
    "    # Create a dataloader that samples uniformly with replacement. Pass a generator and a\n",
    "    # worker_init_fn to ensure reproducibility\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "\n",
    "    # Create a random sampler and a batch sampler to sample the constraints uniformly with replacement\n",
    "    random_sampler = RandomSampler(dataset, replacement=False, generator=g)\n",
    "    batch_sampler = BatchSampler(random_sampler, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "    return DataLoader(dataset, sampler=batch_sampler, batch_size=None)\n",
    "\n",
    "\n",
    "class MinNormWithLinearConstraints(cooper.ConstrainedMinimizationProblem):\n",
    "    \"\"\"Min-norm problem with linear equality constraints.\"\"\"\n",
    "\n",
    "    def __init__(self, num_equations: int) -> None:\n",
    "        super().__init__()\n",
    "        # Create a constraint for the equality constraints. We use a sparse constraint\n",
    "        # to be able to update the multipliers only with the observed constraints (i.e. the\n",
    "        # ones that are active in the current batch)\n",
    "        constraint_type = cooper.ConstraintType.EQUALITY\n",
    "        multiplier = cooper.multipliers.IndexedMultiplier(num_constraints=num_equations, device=DEVICE)\n",
    "        self.eq_constraint = cooper.Constraint(\n",
    "            constraint_type=constraint_type, formulation_type=cooper.formulations.Lagrangian, multiplier=multiplier\n",
    "        )\n",
    "\n",
    "    def compute_cmp_state(\n",
    "        self, x: torch.Tensor, sampled_equations: torch.Tensor, sampled_RHS: torch.Tensor, indices: torch.Tensor\n",
    "    ) -> cooper.CMPState:\n",
    "        loss = torch.linalg.vector_norm(x) ** 2\n",
    "        violation = (sampled_equations @ x - sampled_RHS).view(-1)\n",
    "        constraint_state = cooper.ConstraintState(violation=violation, constraint_features=indices)\n",
    "        return cooper.CMPState(loss=loss, observed_constraints={self.eq_constraint: constraint_state})\n",
    "\n",
    "\n",
    "def run_experiment(\n",
    "    num_equations, num_variables, batch_size, num_steps, primal_lr, dual_lr, data_seed=135, exp_seed=246\n",
    "):\n",
    "    # Create a random linear system\n",
    "    A, b, x_optim = create_linear_system(num_equations, num_variables, seed=data_seed)\n",
    "    optimal_sq_norm = torch.linalg.vector_norm(x_optim).item() ** 2\n",
    "    all_indices = torch.arange(num_equations, device=DEVICE)\n",
    "\n",
    "    torch.manual_seed(seed=exp_seed)\n",
    "    linear_system_dataset = LinearConstraintDataset(A, b)\n",
    "    constraint_loader = instantiate_dataloader(dataset=linear_system_dataset, batch_size=batch_size, seed=exp_seed)\n",
    "\n",
    "    # Define the problem with the constraint\n",
    "    cmp = MinNormWithLinearConstraints(num_equations=num_equations)\n",
    "\n",
    "    # Randomly initialize the primal variable and instantiate the optimizers\n",
    "    x = torch.nn.Parameter(torch.rand(num_variables, 1, device=DEVICE) / np.sqrt(num_variables))\n",
    "\n",
    "    primal_optimizer = torch.optim.SGD([x], lr=primal_lr, momentum=0.9)\n",
    "    dual_optimizer = torch.optim.SGD(cmp.dual_parameters(), lr=dual_lr, maximize=True, foreach=False)\n",
    "    cooper_optimizer = cooper.optim.SimultaneousOptimizer(\n",
    "        primal_optimizers=primal_optimizer, dual_optimizers=dual_optimizer, cmp=cmp\n",
    "    )\n",
    "\n",
    "    state_history = {\"step\": [], \"relative_norm\": [], \"multipliers\": [], \"x_gap\": [], \"max_abs_violation\": []}\n",
    "    step_ix = 0\n",
    "\n",
    "    while step_ix < num_steps:\n",
    "        for sampled_equations, sampled_RHS, indices in constraint_loader:\n",
    "            step_ix += 1\n",
    "\n",
    "            sampled_equations, sampled_RHS = sampled_equations.to(DEVICE), sampled_RHS.to(DEVICE)\n",
    "            indices = indices.to(DEVICE)\n",
    "            compute_cmp_state_kwargs = {\n",
    "                \"x\": x,\n",
    "                \"sampled_equations\": sampled_equations,\n",
    "                \"sampled_RHS\": sampled_RHS,\n",
    "                \"indices\": indices,\n",
    "            }\n",
    "            _, cmp_state, _, _ = cooper_optimizer.roll(compute_cmp_state_kwargs=compute_cmp_state_kwargs)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                full_violation = A @ x - b\n",
    "                max_abs_violation = torch.linalg.vector_norm(full_violation, ord=np.inf)\n",
    "\n",
    "            state_history[\"step\"].append(step_ix)\n",
    "            state_history[\"relative_norm\"].append(cmp_state.loss.item() / optimal_sq_norm)\n",
    "            state_history[\"multipliers\"].append(cmp.eq_constraint.multiplier(all_indices).cpu().tolist())\n",
    "            state_history[\"x_gap\"].append(torch.linalg.vector_norm(x - x_optim, ord=np.inf).cpu().item())\n",
    "            state_history[\"max_abs_violation\"].append(max_abs_violation.cpu().item())\n",
    "\n",
    "    return state_history\n",
    "\n",
    "\n",
    "def plot_results(state_histories) -> None:\n",
    "    _, ax = plt.subplots(len(state_histories), 4, figsize=(20, len(state_histories) * 4))\n",
    "\n",
    "    for exp_id, (exp_label, state_history) in enumerate(state_histories):\n",
    "        [ax[exp_id, _].set_xlabel(\"Step\") for _ in range(4)]\n",
    "\n",
    "        ax[exp_id, 0].set_ylabel(exp_label)\n",
    "\n",
    "        ax[exp_id, 0].plot(state_history[\"step\"], state_history[\"relative_norm\"])\n",
    "        ax[exp_id, 0].axhline(1, color=\"red\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "        ax[exp_id, 1].plot(state_history[\"step\"], np.stack(state_history[\"multipliers\"]).squeeze(), alpha=0.5)\n",
    "\n",
    "        ax[exp_id, 2].plot(state_history[\"step\"], np.stack(state_history[\"x_gap\"]).squeeze())\n",
    "        ax[exp_id, 2].set_yscale(\"log\")\n",
    "        ax[exp_id, 2].axhline(0, color=\"red\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "        ax[exp_id, 3].plot(state_history[\"step\"], np.stack(state_history[\"max_abs_violation\"]).squeeze())\n",
    "        ax[exp_id, 3].set_yscale(\"log\")\n",
    "        ax[exp_id, 3].axhline(0, color=\"red\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "    ax[0, 0].set_title(r\"$\\|x\\|^2_2 $ / $\\|x^*\\|^2_2$\")\n",
    "    ax[0, 1].set_title(\"Multipliers\")\n",
    "    ax[0, 2].set_title(r\"$\\|x - x^*\\|_\\infty$\")\n",
    "    ax[0, 3].set_title(r\"$\\|Ax - b\\|_\\infty$\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "state_histories = []\n",
    "num_equations, num_variables = 100, 500\n",
    "experiment_kwargs = {\"num_steps\": 5000, \"primal_lr\": 1e-3, \"dual_lr\": 1e-2}\n",
    "\n",
    "for constraint_frequency in [1.0, 0.5, 0.25]:\n",
    "    batch_size = int(constraint_frequency * num_equations)\n",
    "    state_history = run_experiment(num_equations, num_variables, batch_size, **experiment_kwargs)\n",
    "    state_histories.append((f\"Constraint Frequency: {constraint_frequency * 100}%\", state_history))\n",
    "\n",
    "plot_results(state_histories)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
