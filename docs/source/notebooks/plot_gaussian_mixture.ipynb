{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear classification with rate constraints.\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cooper-org/cooper/blob/main/docs/source/notebooks/plot_gaussian_mixture.ipynb)\n",
    "\n",
    "\n",
    ":::{note}\n",
    "This example highlights the use of proxy constraints {cite:t}`cotter2019proxy`. Proxy\n",
    "constraints allow using different constraint violations for updating the primal and\n",
    "dual variables. They are useful when the true constraint is non-differentiable, but\n",
    "there exists a differentiable surrogate that is aligned with the original constraint.\n",
    "This example is based on Fig. 2 of {cite:t}`cotter2019proxy`.\n",
    "\n",
    "By default, Cooper uses the provided violation to update both the primal and dual.\n",
    "To use proxy constraints, the user must provide a `strict_violation` in the\n",
    "`ConstraintState` object. The `strict_violation` is used to update the dual variables,\n",
    "while the `violation` is used to update the primal variables.\n",
    ":::\n",
    "\n",
    "In this example we consider a linear classification problem on a synthetically generated\n",
    "mixture of Gaussians. We constrain the model to predict at least 70% of the training\n",
    "points as class blue (class 0). The optimization problem is defined as follows:\n",
    "\n",
    "$$\n",
    "min_{w, b} \\mathbb{E}_{(x, y) \\sim \\mathcal{D}} \\left[ \\ell(w^T x + b, y) \\right] s.t. \\mathbb{E}_{(x, y) \\sim \\mathcal{D}} \\mathbb{1}_{\\sigma(w^T x + b) \\leq 0} \\geq 0.7\n",
    "$$\n",
    "\n",
    "where $\\ell$ is the binary cross-entropy loss, $w$ and $b$ are the\n",
    "weights and bias of the linear model, and $x$ and $y$ are the input and\n",
    "target labels. The expectation is computed over the data distribution\n",
    "$\\mathcal{D}$.\n",
    "\n",
    "Note that this constraint is not continuous on the model parameters, and so it is not\n",
    "differentiable. The typical Lagrangian approach is not applicable in this setting as\n",
    "we can not compute the derivatives of the constraint.\n",
    "\n",
    "A possible approach to deal with this difficulty is to retain the Lagrangian\n",
    "formulation, but replace the constraints with differentiable approximations or\n",
    "surrogates. However, changing the constraint functions can result in an over- or\n",
    "under-constrained version of the problem (as illustrated in this tutorial).\n",
    "\n",
    "{cite:t}`cotter2019proxy` propose a *proxy-Lagrangian formulation*, in which the\n",
    "non-differentiable constraints are relaxed *only when necessary*. In other\n",
    "words, the non-differentiable constraint functions are used to compute the\n",
    "Lagrangian and constraint violations (and thus the to update the Lagrange multipliers),\n",
    "while the surrogates are used to compute gradients of the Lagrangian with\n",
    "respect to the parameters of the model.\n",
    "\n",
    "**Surrogate.** The surrogate considered in this tutorial is the following:\n",
    "\n",
    "$$\n",
    "P(\\hat{y}=0) \\geq 0.7\n",
    "$$\n",
    "\n",
    "where $P(\\hat{y}=0) = \\mathbb{E}_{(x, y) \\sim \\mathcal{D}} P(\\hat{y}=0 | x) = \\mathbb{E}_{(x, y) \\sim \\mathcal{D}} 1 - \\sigma(w^T x + b)$\n",
    "is the proportion of points predicted as class 0 by the model.\n",
    "\n",
    "Note that the surrogate is differentiable with respect to the model parameters, and it\n",
    "is aligned with the original constraint: an increase in the probability of predicting\n",
    "class 0 leads to an increase in the proportion of points predicted as class 0.\n",
    "\n",
    "**Results.** The plot shows the decision boundary of the linear model trained with\n",
    "three different formulations: unconstrained, constrained with the surrogate constraint,\n",
    "and constrained with proxy constraints.\n",
    "\n",
    "* The proportion of points predicted as class 0 when training without constraints is\n",
    "50%. This is to be expected, as the model is optimizing the loss over a balanced and\n",
    "(almost) separable dataset.\n",
    "* The middle plot shows the decision boundary of the model trained with the surrogate\n",
    "constraint. The proportion of points predicted as class 0 is 66%, which is below the\n",
    "desired 70%. This is due to the relaxation of the constraint, which is different\n",
    "from the original constraint.\n",
    "* The rightmost plot shows the decision boundary of the model trained with the proxy\n",
    "constraint. The proportion of points predicted as class 0 is 70%, thus yielding a\n",
    "feasible solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install cooper-optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import cooper\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "\n",
    "def generate_mog_dataset():\n",
    "    \"\"\"Generate a MoG dataset on 2D, with two classes.\"\"\"\n",
    "    n_samples_per_class = 100\n",
    "    dim = 2\n",
    "    n_gaussians = 4\n",
    "    means = [(0, 1), (-1, 0), (0, -1), (1, 0)]\n",
    "    means = [torch.tensor(m) for m in means]\n",
    "    var = 0.05\n",
    "\n",
    "    inputs, labels = [], []\n",
    "\n",
    "    for idx in range(n_gaussians):\n",
    "        # Generate input data by mu + x @ sqrt(cov)\n",
    "        cov = np.sqrt(var) * torch.eye(dim)  # Diagonal covariance matrix\n",
    "        mu = means[idx]\n",
    "        inputs.append(mu + torch.randn(n_samples_per_class, dim) @ cov)\n",
    "\n",
    "        # Labels\n",
    "        labels.append(torch.tensor(n_samples_per_class * [1.0 if idx < 2 else 0.0]))\n",
    "\n",
    "    return torch.cat(inputs, dim=0), torch.cat(labels, dim=0)\n",
    "\n",
    "\n",
    "def plot_pane(ax, inputs, x1, x2, achieved_const, titles, colors):\n",
    "    const_str = str(np.round(achieved_const, 0)) + \"%\"\n",
    "    ax.scatter(*torch.transpose(inputs, 0, 1), color=colors)\n",
    "    ax.plot(x1, x2, color=\"gray\", linestyle=\"--\")\n",
    "    ax.fill_between(x1, -2, x2, color=blue, alpha=0.1)\n",
    "    ax.fill_between(x1, x2, 2, color=red, alpha=0.1)\n",
    "\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_xlim(-2, 2)\n",
    "    ax.set_ylim(-2, 2)\n",
    "    ax.set_title(titles[idx] + \" - Pred. Blue: \" + const_str)\n",
    "\n",
    "\n",
    "class UnconstrainedMixtureSeparation(cooper.ConstrainedMinimizationProblem):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def compute_cmp_state(self, model, inputs, targets):\n",
    "        logits = model(inputs)\n",
    "        loss = torch.nn.functional.binary_cross_entropy_with_logits(logits.flatten(), targets)\n",
    "        return cooper.CMPState(loss=loss)\n",
    "\n",
    "\n",
    "class MixtureSeparation(cooper.ConstrainedMinimizationProblem):\n",
    "    \"\"\"Implements CMP for separating the MoG dataset with a linear predictor.\n",
    "\n",
    "    Args:\n",
    "        use_strict_constraints: Flag to use proxy-constraints. If ``False``, we use a\n",
    "            hinge relaxation both for updating the Lagrange multipliers and for updating\n",
    "            the model parameters. If ``True``, we use the hinge relaxation only for the\n",
    "            model parameters, and the true rate of blue predictions for updating the\n",
    "            Lagrange multipliers. Defaults to ``False``.\n",
    "        constraint_level: Minimum proportion of points to be predicted as belonging to\n",
    "            the blue class. Ignored when ``is_constrained==False``. Defaults to ``0.7``.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, use_strict_constraints: bool = False, constraint_level: float = 0.7):\n",
    "        super().__init__()\n",
    "\n",
    "        constraint_type = cooper.ConstraintType.INEQUALITY\n",
    "        multiplier = cooper.multipliers.DenseMultiplier(num_constraints=1)\n",
    "        self.rate_constraint = cooper.Constraint(\n",
    "            constraint_type=constraint_type, formulation_type=cooper.formulations.Lagrangian, multiplier=multiplier\n",
    "        )\n",
    "\n",
    "        self.constraint_level = constraint_level\n",
    "        self.use_strict_constraints = use_strict_constraints\n",
    "\n",
    "    def compute_cmp_state(self, model, inputs, targets):\n",
    "        logits = model(inputs)\n",
    "        loss = torch.nn.functional.binary_cross_entropy_with_logits(logits.flatten(), targets)\n",
    "\n",
    "        # Hinge approximation of the rate\n",
    "        probs = torch.sigmoid(logits)\n",
    "\n",
    "        # Surrogate \"constraint\": prob_0 >= constraint_level -> constraint_level - prob_0 <= 0\n",
    "        differentiable_violation = self.constraint_level - torch.mean(1 - probs)\n",
    "\n",
    "        if self.use_strict_constraints:\n",
    "            # Use the true rate of blue predictions as the constraint\n",
    "            classes = logits >= 0.0\n",
    "            prop_0 = torch.sum(classes == 0) / targets.numel()\n",
    "\n",
    "            # Constraint: prop_0 >= constraint_level -> constraint_level - prop_0 <= 0\n",
    "            strict_violation = self.constraint_level - prop_0\n",
    "\n",
    "            constraint_state = cooper.ConstraintState(\n",
    "                violation=differentiable_violation, strict_violation=strict_violation\n",
    "            )\n",
    "        else:\n",
    "            constraint_state = cooper.ConstraintState(violation=differentiable_violation)\n",
    "\n",
    "        return cooper.CMPState(loss=loss, observed_constraints={self.rate_constraint: constraint_state})\n",
    "\n",
    "\n",
    "def train(problem_name, inputs, targets, num_iters=5000, lr=1e-2, constraint_level=0.7):\n",
    "    is_constrained = \"unconstrained\" not in problem_name.lower()\n",
    "    use_strict_constraints = \"proxy\" in problem_name.lower()\n",
    "\n",
    "    model = torch.nn.Linear(2, 1)\n",
    "    primal_optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.7)\n",
    "\n",
    "    if is_constrained:\n",
    "        cmp = MixtureSeparation(use_strict_constraints, constraint_level)\n",
    "        dual_optimizer = torch.optim.SGD(cmp.dual_parameters(), lr=lr, momentum=0.7, maximize=True)\n",
    "        cooper_optimizer = cooper.optim.SimultaneousOptimizer(\n",
    "            primal_optimizers=primal_optimizer, dual_optimizers=dual_optimizer, cmp=cmp\n",
    "        )\n",
    "    else:\n",
    "        cmp = UnconstrainedMixtureSeparation()\n",
    "        cooper_optimizer = cooper.optim.UnconstrainedOptimizer(primal_optimizers=primal_optimizer, cmp=cmp)\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        cooper_optimizer.roll(compute_cmp_state_kwargs={\"model\": model, \"inputs\": inputs, \"targets\": targets})\n",
    "\n",
    "    # Number of elements predicted as class 0 in the train set after training\n",
    "    logits = model(inputs)\n",
    "    pred_classes = logits >= 0.0\n",
    "    prop_0 = torch.sum(pred_classes == 0) / targets.numel()\n",
    "\n",
    "    return model, 100 * prop_0.item()\n",
    "\n",
    "\n",
    "# Plot configs\n",
    "titles = [\"Unconstrained\", \"Const w/ Surrogate\", \"Proxy Const\"]\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Data and training configs\n",
    "inputs, labels = generate_mog_dataset()\n",
    "constraint_level = 0.7\n",
    "lr = 2e-2\n",
    "num_iters = 5000\n",
    "\n",
    "for idx, name in enumerate(titles):\n",
    "    model, achieved_const = train(name, inputs, labels, lr=lr, num_iters=num_iters, constraint_level=constraint_level)\n",
    "\n",
    "    # Compute decision boundary\n",
    "    weight, bias = model.weight.data.flatten().numpy(), model.bias.data.numpy()\n",
    "    x1 = np.linspace(-2, 2, 100)\n",
    "    x2 = (-1 / weight[1]) * (weight[0] * x1 + bias)\n",
    "\n",
    "    # Color points according to true label\n",
    "    red, blue = \"#92140C\", \"#0035f5\"\n",
    "    colors = [red if _ == 1 else blue for _ in labels.flatten()]\n",
    "    plot_pane(axs[idx], inputs, x1, x2, achieved_const, titles, colors)\n",
    "\n",
    "fig.suptitle(\"Goal: Predict at least \" + str(constraint_level * 100) + \"% as blue\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
