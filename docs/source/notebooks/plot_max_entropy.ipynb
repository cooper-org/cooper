{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding a maximum entropy (discrete) distribution using the Lagrangian Approach.\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cooper-org/cooper/blob/master/docs/source/notebooks/plot_max_entropy.ipynb)\n",
    "\n",
    "\n",
    "Here we consider a simple convex optimization problem to illustrate how to use\n",
    "**Cooper**. This example is inspired by [this StackExchange question](https://datascience.stackexchange.com/questions/107366/how-do-you-solve-strictly-constrained-optimization-problems-with-pytorch):\n",
    "\n",
    "*I am trying to solve the following problem using PyTorch: given a 6-sided die\n",
    "whose average roll is known to be 4.5, what is the maximum entropy distribution\n",
    "for the faces?*\n",
    "\n",
    "Formally, we want to solve the following optimization problem:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\max_{p} & -\\sum_{i=1}^6 p_i \\log p_i \\\\\n",
    "& \\sum_{i=1}^6 i p_i = 4.5 \\\\\n",
    "\\text{s.t.} & \\sum_{i=1}^6 p_i = 1 \\\\\n",
    "& p_i \\geq 0 \\quad \\forall i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $p$ is the probability distribution over the faces of the die.\n",
    "\n",
    "This example makes use of the $\\nu$PI algorithm for improving the training dynamics of\n",
    "the dual variables. For a detailed explanation of the $\\nu$PI algorithm, see the paper:\n",
    "*On PI Controllers for Updating Lagrange Multipliers in Constrained Optimization* at\n",
    "[ICML 2024](https://icml.cc/virtual/2024/poster/35138)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# %pip install cooper-optim\n",
    "%pip install --index-url https://test.pypi.org/simple/ --no-deps cooper-optim  # TODO: Remove this line when cooper deployed to pypi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import cooper\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class MaximumEntropy(cooper.ConstrainedMinimizationProblem):\n",
    "    def __init__(self, target_mean: float) -> None:\n",
    "        super().__init__()\n",
    "        self.target_mean = target_mean\n",
    "\n",
    "        mean_multiplier = cooper.multipliers.DenseMultiplier(num_constraints=1, device=DEVICE)\n",
    "        sum_multiplier = cooper.multipliers.DenseMultiplier(num_constraints=1, device=DEVICE)\n",
    "\n",
    "        self.mean_constraint = cooper.Constraint(\n",
    "            constraint_type=cooper.ConstraintType.EQUALITY,\n",
    "            formulation_type=cooper.formulations.Lagrangian,\n",
    "            multiplier=mean_multiplier,\n",
    "        )\n",
    "        self.sum_constraint = cooper.Constraint(\n",
    "            constraint_type=cooper.ConstraintType.EQUALITY,\n",
    "            formulation_type=cooper.formulations.Lagrangian,\n",
    "            multiplier=sum_multiplier,\n",
    "        )\n",
    "\n",
    "        # For simple non-negativity constraints, we use projection\n",
    "\n",
    "    def compute_cmp_state(self, log_probs: torch.Tensor) -> cooper.CMPState:\n",
    "        probs = torch.exp(log_probs)\n",
    "        entropy = -torch.sum(probs * log_probs)\n",
    "\n",
    "        # Equality constraints for proper normalization and mean constraint\n",
    "        mean = torch.sum(probs * torch.arange(1, len(probs) + 1, device=DEVICE))\n",
    "\n",
    "        sum_constraint_violation = cooper.ConstraintState(violation=torch.sum(probs) - 1)\n",
    "        mean_constraint_violation = cooper.ConstraintState(violation=mean - self.target_mean)\n",
    "\n",
    "        observed_constraints = {\n",
    "            self.sum_constraint: sum_constraint_violation,\n",
    "            self.mean_constraint: mean_constraint_violation,\n",
    "        }\n",
    "\n",
    "        # Flip loss sign since we want to *maximize* the entropy\n",
    "        return cooper.CMPState(loss=-entropy, observed_constraints=observed_constraints)\n",
    "\n",
    "\n",
    "# Define the problem with the constraints\n",
    "cmp = MaximumEntropy(target_mean=4.5)\n",
    "\n",
    "# Define the primal parameters and optimizer\n",
    "log_probs = torch.nn.Parameter(torch.log(torch.ones(6, device=DEVICE) / 6))\n",
    "primal_optimizer = torch.optim.SGD([log_probs], lr=3e-2)\n",
    "\n",
    "# We employ the nuPI algorithm for updating the dual variables\n",
    "dual_optimizer = cooper.optim.nuPI(cmp.dual_parameters(), lr=1e-2, Kp=10, maximize=True)\n",
    "\n",
    "cooper_optimizer = cooper.optim.SimultaneousOptimizer(\n",
    "    primal_optimizers=primal_optimizer, dual_optimizers=dual_optimizer, cmp=cmp\n",
    ")\n",
    "\n",
    "state_history = {}\n",
    "for i in range(3000):\n",
    "    _, cmp_state, primal_lagrangian_store, _ = cooper_optimizer.roll(compute_cmp_state_kwargs={\"log_probs\": log_probs})\n",
    "\n",
    "    observed_violations = list(cmp_state.observed_violations())\n",
    "    observed_multipliers = list(primal_lagrangian_store.observed_multiplier_values())\n",
    "    state_history[i] = {\n",
    "        \"loss\": -cmp_state.loss.item(),\n",
    "        \"multipliers\": torch.stack(observed_multipliers).detach(),\n",
    "        \"violation\": torch.stack(observed_violations).detach(),\n",
    "    }\n",
    "\n",
    "\n",
    "# Theoretical solution\n",
    "optimal_prob = torch.tensor([0.05435, 0.07877, 0.1142, 0.1654, 0.2398, 0.3475])\n",
    "optimal_entropy = -torch.sum(optimal_prob * torch.log(optimal_prob))\n",
    "\n",
    "# Generate plots\n",
    "iters, loss_hist, multipliers_hist, violation_hist = zip(\n",
    "    *[(k, v[\"loss\"], v[\"multipliers\"], v[\"violation\"]) for k, v in state_history.items()]\n",
    ")\n",
    "\n",
    "_, (ax0, ax1, ax2) = plt.subplots(1, 3, figsize=(20, 4))\n",
    "\n",
    "ax0.plot(iters, torch.stack(multipliers_hist).squeeze().cpu())\n",
    "ax0.axhline(0.0, c=\"gray\", alpha=0.35)\n",
    "ax0.set_title(\"Multipliers\")\n",
    "\n",
    "ax1.plot(iters, torch.stack(violation_hist).squeeze().cpu(), label=[\"Sum Constraint\", \"Mean Constraint\"])\n",
    "ax1.legend()\n",
    "# Show that defect remains below/at zero\n",
    "ax1.axhline(0.0, c=\"gray\", alpha=0.35)\n",
    "ax1.set_title(\"Constraint Violations\")\n",
    "\n",
    "ax2.plot(iters, loss_hist)\n",
    "# Show optimal entropy is achieved\n",
    "ax2.axhline(optimal_entropy, c=\"gray\", alpha=0.35, linestyle=\"dashed\")\n",
    "ax2.set_title(\"Objective\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
