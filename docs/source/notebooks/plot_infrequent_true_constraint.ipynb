{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding a spectrum-constrained linear transformation between two vectors.\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cooper-org/cooper/blob/main/docs/source/notebooks/plot_infrequent_true_constraint.ipynb)\n",
    "\n",
    "\n",
    ":::{note}\n",
    "This example highlights the use of the flags `contributes_to_primal_update` and\n",
    "`contributes_to_dual_update` in the `ConstraintState` class. These flags are used to\n",
    "specify whether a constraint violation contributes to the primal or dual update. By\n",
    "default, both flags are set to `True`.\n",
    "\n",
    "However, in this example, we update the primal parameters based on a _surrogate_\n",
    "constraint since the true constraint is expensive to compute, and difficult to\n",
    "differentiate. The true constraint is only computed every few iterations, and the\n",
    "multipliers are (infrequently) updated based on the true constraint.\n",
    "\n",
    "Note that this update scheme is similar to the proxy-constraint approach. However, using\n",
    "proxy-constraints via the `strict_violation` argument in the `ConstraintState` class\n",
    "would still require the computation of the true constraint at every iteration. In this\n",
    "example, we avoid this by using the `contributes_to_primal_update` and\n",
    "`contributes_to_dual_update` flags, enabling the update of the primal and dual variables\n",
    "at different frequencies.\n",
    ":::\n",
    "\n",
    "Consider the problem of finding the matrix $X$ that transforms a vector $y$\n",
    "so as to minimize the mean squared error between $Xy$ and another vector $z$,\n",
    "under a constraint on the geometric mean of the singular values of $X$. Formally,\n",
    "\n",
    "$$\n",
    "\\min_{X}  \\,\\, \\Vert Xy - z \\Vert_2^2  \\,\\, \\text{ such that } \\,\\, \\prod_{i=1}^r \\sigma_i(X) = c^r\n",
    "$$\n",
    "\n",
    "where $X \\in \\mathbb{R}^{m \\times n}$, $y \\in \\mathbb{R}^m$,\n",
    "$z \\in \\mathbb{R}^n$, $r = \\min\\{m, n\\}$, $\\sigma_i(X)$ denotes the\n",
    "$i$-th singular value of $X$, and $c$ is a constant.\n",
    "\n",
    "We calculate the geometric mean of the singular values of $X$ by first computing\n",
    "the singular value decomposition of $X$. Note that the SVD decomposition is\n",
    "relatively expensive. However, the *arithmetic* mean of the squared singular values of\n",
    "$X$ can be computed cheaply as it corresponds to the trace of $X X^T$.\n",
    "\n",
    "Therefore, we can use the arithmetic mean as a surrogate for the true constraint on the\n",
    "geometric mean of the singular values of X. While this choice of surrogate is not\n",
    "guaranteed to produce the same solution as the true constraint, the tutorial illustrates\n",
    "it is a good practical heuristic.\n",
    "\n",
    "This example illustrates the ability to update the primal and dual variables at\n",
    "different frequencies. Here, we make use of the cheap surrogate constraint to update the\n",
    "primal variables at every iteration, while the multipliers are updated using the \\_true\\_\n",
    "constraint which is only observed sporadically. Note how the multiplier value remains\n",
    "constant in-between measurements of the true constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install cooper-optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import cooper\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def create_vectors(dim_y: int, dim_z: int, seed: int = 0):\n",
    "    \"\"\"Create y and z such that Xy = z is true for a well-conditioned matrix X.\"\"\"\n",
    "    torch.manual_seed(seed=seed)\n",
    "\n",
    "    # Create a random linear system with all singular values equal to 1.\n",
    "    U, _, V = torch.linalg.svd(torch.randn(dim_z, dim_y))\n",
    "    S = torch.eye(dim_z, dim_y)\n",
    "    X_true = U @ S @ V.T\n",
    "\n",
    "    y = torch.randn(dim_y, 1)\n",
    "    y = y / torch.linalg.norm(y)\n",
    "\n",
    "    z = X_true @ y\n",
    "    z = z / torch.linalg.norm(z)\n",
    "\n",
    "    y, z = y.to(DEVICE), z.to(DEVICE)\n",
    "\n",
    "    return y, z\n",
    "\n",
    "\n",
    "class MinNormWithSingularValueConstraints(cooper.ConstrainedMinimizationProblem):\n",
    "    \"\"\"Find a matrix X to minimize the error of a linear system, under a constraint on\n",
    "    the geometric mean of the singular values of X.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, y: torch.Tensor, z: torch.Tensor, constraint_level: float = 1.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.y, self.z = y, z\n",
    "        self.r = min(y.shape[0], z.shape[0])\n",
    "        self.constraint_level = constraint_level\n",
    "\n",
    "        # Creating a constraint with a single equality constraint\n",
    "        constraint_type = cooper.ConstraintType.EQUALITY\n",
    "        multiplier = cooper.multipliers.DenseMultiplier(num_constraints=1, device=DEVICE)\n",
    "        self.sv_constraint = cooper.Constraint(\n",
    "            constraint_type=constraint_type, formulation_type=cooper.formulations.Lagrangian, multiplier=multiplier\n",
    "        )\n",
    "\n",
    "    def loss_fn(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute the MSE loss function for a given X.\"\"\"\n",
    "        return torch.linalg.norm(X @ self.y - self.z).pow(2) / 2\n",
    "\n",
    "    def compute_arithmetic_mean(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute the arithmetic mean of the squared singular values of X.\"\"\"\n",
    "        # We use the *arithmetic* mean of the squared singular values of X as a\n",
    "        # surrogate for the true constraint given by the geometric mean of the singular\n",
    "        # values of X.\n",
    "        # Since the surrogate is only used to compute gradients, there is no need to set\n",
    "        # a constraint level offset.\n",
    "        # This is equivalent to computing the trace of X * X^T (and dividing by r)\n",
    "        return torch.einsum(\"ij,ij->\", X, X) / self.r\n",
    "\n",
    "    @staticmethod\n",
    "    @torch.no_grad()\n",
    "    def compute_geometric_mean(X: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.linalg.svdvals(X).prod()\n",
    "\n",
    "    def compute_surrogate_constraint(self, X: torch.Tensor) -> cooper.CMPState:\n",
    "        \"\"\"Compute the (differentiable) surrogate violation for the primal update.\"\"\"\n",
    "        # The `contributes_to_primal_update=True` and `contributes_to_dual_update=False`\n",
    "        # flags indicate that the constraint is used to update the primal variables only.\n",
    "        constraint_state = cooper.ConstraintState(\n",
    "            violation=self.compute_arithmetic_mean(X),\n",
    "            contributes_to_primal_update=True,\n",
    "            contributes_to_dual_update=False,\n",
    "        )\n",
    "\n",
    "        return constraint_state\n",
    "\n",
    "    def compute_true_constraint(self, X: torch.Tensor) -> cooper.CMPState:\n",
    "        \"\"\"Compute the non-differentiable constraint to update the multipliers.\"\"\"\n",
    "        # The `contributes_to_primal_update=False` and `contributes_to_dual_update=True`\n",
    "        # flags indicate that the constraint is used to update the dual variables only.\n",
    "        constraint_state = cooper.ConstraintState(\n",
    "            violation=self.compute_geometric_mean(X) - self.constraint_level,\n",
    "            contributes_to_primal_update=False,\n",
    "            contributes_to_dual_update=True,\n",
    "        )\n",
    "\n",
    "        return constraint_state\n",
    "\n",
    "    def compute_cmp_state(self, X: torch.Tensor, is_true_constraint: bool) -> cooper.CMPState:\n",
    "        objective = self.loss_fn(X)\n",
    "\n",
    "        if is_true_constraint:\n",
    "            constraint_state = self.compute_true_constraint(X)\n",
    "        else:\n",
    "            constraint_state = self.compute_surrogate_constraint(X)\n",
    "\n",
    "        return cooper.CMPState(loss=objective, observed_constraints={self.sv_constraint: constraint_state})\n",
    "\n",
    "\n",
    "def run_experiment(dim_y, dim_z, constraint_level, max_iter, tolerance, freq_for_dual_update, primal_lr, dual_lr):\n",
    "    y, z = create_vectors(dim_y=dim_y, dim_z=dim_z, seed=0)\n",
    "\n",
    "    X = np.random.randn(dim_z, dim_y) / np.sqrt(dim_y * dim_z)\n",
    "\n",
    "    # Creating X as a tensor from scratch for it to be a leaf tensor\n",
    "    X = torch.tensor(X, requires_grad=True, device=DEVICE, dtype=torch.float32)\n",
    "\n",
    "    cmp = MinNormWithSingularValueConstraints(y=y, z=z, constraint_level=constraint_level)\n",
    "    primal_optimizer = torch.optim.SGD([X], lr=primal_lr)\n",
    "    dual_optimizer = torch.optim.SGD(cmp.dual_parameters(), lr=dual_lr, maximize=True, foreach=False)\n",
    "    cooper_optimizer = cooper.optim.AlternatingDualPrimalOptimizer(\n",
    "        primal_optimizers=primal_optimizer, cmp=cmp, dual_optimizers=dual_optimizer\n",
    "    )\n",
    "\n",
    "    # Initial values of the loss, trace and geometric mean\n",
    "    with torch.no_grad():\n",
    "        state_history = {\n",
    "            \"loss\": [cmp.loss_fn(X).item()],\n",
    "            \"arithmetic_mean\": [cmp.compute_arithmetic_mean(X).item()],\n",
    "            \"geometric_mean\": [cmp.compute_geometric_mean(X).item()],\n",
    "            \"multiplier_values\": [cmp.sv_constraint.multiplier.weight.item()],\n",
    "        }\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        prev_X = X.clone().detach()\n",
    "\n",
    "        cooper_optimizer.roll({\"X\": X, \"is_true_constraint\": (it % freq_for_dual_update) == 0})\n",
    "\n",
    "        if prev_X.allclose(X, atol=tolerance):\n",
    "            break\n",
    "\n",
    "        with torch.no_grad():\n",
    "            state_history[\"loss\"].append(cmp.loss_fn(X).item())\n",
    "            state_history[\"arithmetic_mean\"].append(cmp.compute_arithmetic_mean(X).item())\n",
    "            state_history[\"geometric_mean\"].append(cmp.compute_geometric_mean(X).item())\n",
    "            state_history[\"multiplier_values\"].append(cmp.sv_constraint.multiplier.weight.item())\n",
    "\n",
    "    return state_history\n",
    "\n",
    "\n",
    "def plot_results(state_history, constraint_level):\n",
    "    _, ax = plt.subplots(2, 2, figsize=(12, 6))\n",
    "\n",
    "    ax[0, 0].plot(state_history[\"loss\"])\n",
    "    ax[0, 0].set_ylabel(\"MSE Loss\")\n",
    "    ax[0, 0].set_yscale(\"log\")\n",
    "    ax[0, 0].grid(True, which=\"both\", alpha=0.3)\n",
    "\n",
    "    ax[0, 1].plot(state_history[\"arithmetic_mean\"])\n",
    "    ax[0, 1].set_ylabel(\"Arith. mean sq. singular values\")\n",
    "\n",
    "    ax[1, 0].plot(state_history[\"geometric_mean\"])\n",
    "    ax[1, 0].set_ylabel(\"Geometric mean\")\n",
    "    # Horizontal line at desired `constraint_level`\n",
    "    ax[1, 0].axhline(constraint_level, color=\"red\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "    ax[1, 1].plot(state_history[\"multiplier_values\"])\n",
    "    ax[1, 1].set_ylabel(\"Multiplier\")\n",
    "    ax[1, 1].axhline(0, color=\"red\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "    for ax_ in ax.flatten():\n",
    "        ax_.set_xlabel(\"Iteration\")\n",
    "        for line in ax_.get_lines():\n",
    "            line.set_linewidth(2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "dim_y, dim_z = 4, 4\n",
    "constraint_level = 1.0 ** min(dim_y, dim_z)\n",
    "primal_lr, dual_lr = 3e-2, 1e-1\n",
    "freq_for_dual_update = 100\n",
    "max_iter, tolerance = 50_000, 1e-6\n",
    "\n",
    "\n",
    "state_history = run_experiment(\n",
    "    dim_y=dim_y,\n",
    "    dim_z=dim_z,\n",
    "    constraint_level=constraint_level,\n",
    "    max_iter=max_iter,\n",
    "    tolerance=tolerance,\n",
    "    freq_for_dual_update=freq_for_dual_update,\n",
    "    primal_lr=primal_lr,\n",
    "    dual_lr=dual_lr,\n",
    ")\n",
    "plot_results(state_history, constraint_level)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
