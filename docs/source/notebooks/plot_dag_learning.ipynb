{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning a Direct Acyclic Graph (DAG) on Data\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cooper-org/cooper/blob/master/docs/source/notebooks/plot_dag_learning.ipynb)\n",
    "\n",
    "\n",
    "Consider the problem of learning a Directed Acyclic Graph (DAG) on data. This is a common problem in causal inference, where we are interested in learning the causal relationships between variables. In this notebook, we will demonstrate how to learn a DAG on data using a {py:class}`~cooper.formulations.QuadraticPenalty` formulation in **Cooper**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# %pip install cooper-optim\n",
    "%pip install --index-url https://test.pypi.org/simple/ --no-deps cooper-optim  # TODO: Remove this line when cooper deployed to pypi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "import cooper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a $d$-dimensional random vector ${X_1, X_2, ..., X_d}$. Given $n$ observations of the random vector $X \\in \\mathbb{R}^{n \\times d}$, we are interested in learning a DAG $G = (V, E)$ whose edges represent the dependencies between the variables. We model the DAG via an adjacency matrix $A \\in \\{0, 1\\}^{d \\times d}$, where $A_{ij} = 1$ if there is an edge from $X_i$ to $X_j$ and $A_{ij} = 0$ otherwise.\n",
    "\n",
    "This problem can be formulated as the following optimization problem:\n",
    "\\begin{equation}\n",
    "\\min_{A \\in \\{0, 1\\}^{d \\times d}} \\left\\| X - XA \\right\\|_F^2 + \\lambda \\|A\\|_1,\n",
    "\\quad s.t. \\quad A \\text{ is acyclic},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\| \\cdot \\|_F$ is the Frobenius norm, $\\lambda$ is a regularization parameter aimed at encougaing sparsity in the learned DAG, and the constraint ensures that the learned graph is acyclic.\n",
    "\n",
    "{cite:t}`NOTEARS` show that the acyclicity constraint can be formulated as $\\text{tr}(e^{A}) = d$, where $e^{A}$ is the matrix exponential of $A$. This yields the following optimization problem:\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{A \\in \\{0, 1\\}^{d \\times d}} \\left\\| X - XA \\right\\|_F^2 + \\lambda \\|A\\|_1,\n",
    "\\quad s.t. \\quad \\text{tr}(e^{A}) = d.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generative process we use is this:\n",
    "\n",
    "$X_i \\leftarrow \\sum_{j \\in \\pi_i} X_j + \\epsilon_i$\n",
    "\n",
    "where $\\pi_i$ is the set of parents of $X_i$, and $\\epsilon_i$ is a noise term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "def generate_data(n: int, d: int, n_causes: int, noise_std: float, device: torch.device):\n",
    "    \"\"\"Generate data from a linear structural equation model with Gaussian noise.\n",
    "    The\n",
    "\n",
    "    Args:\n",
    "        n: number of samples\n",
    "        d: number of features\n",
    "        n_causes: number of roots in the graph\n",
    "        noise_std: standard deviation of the noise\n",
    "        device: torch.device\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: X of shape (n, d)\n",
    "        torch.Tensor: A of shape (d, d)\n",
    "    \"\"\"\n",
    "    assert n_causes <= d\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Generate the adjacency matrix\n",
    "    # --------------------------------------------\n",
    "\n",
    "    # Rows are nodes, columns are parents\n",
    "    A = torch.zeros(d, d, device=device)\n",
    "\n",
    "    for i in range(n_causes, d):\n",
    "        parents = torch.randperm(i)[: np.random.randint(1, i)]\n",
    "        A[i, parents] = 1\n",
    "\n",
    "    assert torch.trace(torch.linalg.matrix_exp(A)).item() == d, \"A is not a DAG\"\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Sample data\n",
    "    # --------------------------------------------\n",
    "\n",
    "    noise = noise_std * torch.randn(n, d, device=device)\n",
    "    X = torch.zeros(n, d, device=device)\n",
    "\n",
    "    for i in range(d):\n",
    "        parents = torch.nonzero(A[i]).flatten()\n",
    "        X[:, i] = X[:, parents].sum(dim=1) + noise[:, i]\n",
    "\n",
    "    # Improve conditioning\n",
    "    X /= math.sqrt(d)\n",
    "\n",
    "    return X, A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the Problem\n",
    "\n",
    "We will use the {py:class}`~cooper.formulations.QuadraticPenalty` formulation to solve the problem. This leads to the following formulation of the constrained optimization problem:\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{A \\in \\{0, 1\\}^{d \\times d}} \\left\\| X - XA \\right\\|_F^2 + \\lambda \\|A\\|_1 + \\frac{c}{2}[\\text{tr}(e^{A}) - d]^2,\n",
    "\\end{equation}\n",
    "\n",
    "where $c$ is a penalty parameter. We will schedule the penalty parameter $c$ to increase over time in order to enforce the acyclicity constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 10\n",
    "N = 1000\n",
    "N_CAUSES = 3\n",
    "NOISE_STD = 1e-1\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Generate data\n",
    "X, A_TRUE = generate_data(N, D, N_CAUSES, NOISE_STD, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConstrainedMinimizationProblem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAGLearning(cooper.ConstrainedMinimizationProblem):\n",
    "    def __init__(self, X: torch.Tensor, lmbda: float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.X = X\n",
    "        self.n, self.d = X.shape\n",
    "        self.lmbda = lmbda\n",
    "\n",
    "        penalty_coefficient = cooper.penalty_coefficients.DensePenaltyCoefficient(\n",
    "            init=torch.tensor(1.0, device=X.device),\n",
    "        )\n",
    "        self.constraint = cooper.Constraint(\n",
    "            constraint_type=cooper.ConstraintType.EQUALITY,\n",
    "            formulation_type=cooper.formulations.QuadraticPenalty,\n",
    "            penalty_coefficient=penalty_coefficient,\n",
    "        )\n",
    "\n",
    "    def compute_cmp_state(self, A: torch.Tensor) -> cooper.CMPState:\n",
    "        loss = torch.linalg.norm(self.X - self.X @ torch.sigmoid(A.T), ord=\"fro\") ** 2\n",
    "        loss += self.lmbda * torch.linalg.norm(torch.sigmoid(A), ord=1)\n",
    "\n",
    "        constraint_value = torch.trace(torch.linalg.matrix_exp(A)) - self.d\n",
    "        constraint_state = cooper.ConstraintState(violation=constraint_value)\n",
    "\n",
    "        return cooper.CMPState(loss=loss, observed_constraints={self.constraint: constraint_state})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.nn.Parameter(torch.randn(D, D, device=DEVICE) / math.sqrt(D))\n",
    "\n",
    "LMBDA = 1e-1\n",
    "PRIMAL_LR = 1e-5\n",
    "MOMENTUM = 0.9\n",
    "N_STEPS = 1_000\n",
    "\n",
    "cmp = DAGLearning(X, LMBDA)\n",
    "\n",
    "primal_optimizer = torch.optim.SGD([A], lr=PRIMAL_LR, momentum=MOMENTUM)\n",
    "constrained_optimizer = cooper.optim.UnconstrainedOptimizer(cmp=cmp, primal_optimizers=primal_optimizer)\n",
    "\n",
    "# Multiply the penalty coefficient by `growth_factor` if the constraint is violated\n",
    "# by more than `violation_tolerance`\n",
    "penalty_scheduler = cooper.penalty_coefficients.MultiplicativePenaltyCoefficientUpdater(\n",
    "    growth_factor=1.01,\n",
    "    violation_tolerance=1e-4,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(\"cooper\")\n",
    "for i in range(N_STEPS):\n",
    "    roll_out = constrained_optimizer.roll(compute_cmp_state_kwargs={\"A\": A})\n",
    "\n",
    "    # Set the diagonal to zero to prevent self-loops\n",
    "    A.data.fill_diagonal_(0)\n",
    "\n",
    "    # Update the penalty coefficient\n",
    "    constraint_state = roll_out.cmp_state.observed_constraints[cmp.constraint]\n",
    "    penalty_scheduler.update_penalty_coefficient_(cmp.constraint, constraint_state)\n",
    "\n",
    "    loss = roll_out.loss.item()\n",
    "    violation = constraint_state.violation.item()\n",
    "    primal_lagrangian = roll_out.primal_lagrangian_store.lagrangian.item()\n",
    "    penalty_coefficient_value = cmp.constraint.penalty_coefficient()\n",
    "\n",
    "    if i % (N_STEPS // 10) == 0:\n",
    "        log = (\n",
    "            f\"Step {i}, loss={loss:.4f}, violation={violation:.4f}, penalty_coefficient={penalty_coefficient_value:.4f}\"\n",
    "        )\n",
    "        logger.info(log)\n",
    "\n",
    "final_log = f\"Final loss: {loss:.4f}, violation: {violation:.4f}, penalty_coefficient: {penalty_coefficient_value:.4f}\"\n",
    "logger.info(final_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_adjacency(adjacency, gt_adjacency):\n",
    "    \"\"\"Plot side by side: 1)the learned adjacency matrix, 2)the ground truth adj\n",
    "    matrix and 3)the difference of these matrices\n",
    "    :param np.ndarray adjacency: learned adjacency matrix\n",
    "    :param np.ndarray gt_adjacency: ground truth adjacency matrix\n",
    "    :param str exp_path: path where to save the image\n",
    "    :param str name: additional suffix to add to the image name\n",
    "    \"\"\"\n",
    "    plt.clf()\n",
    "    _, (ax1, ax2, ax3) = plt.subplots(ncols=3, nrows=1)\n",
    "\n",
    "    kwargs = {\"vmin\": -1, \"vmax\": 1, \"cmap\": \"Blues_r\", \"xticklabels\": False, \"yticklabels\": False}\n",
    "    sns.heatmap(adjacency, ax=ax2, cbar=False, **kwargs)\n",
    "    sns.heatmap(gt_adjacency, ax=ax3, cbar=False, **kwargs)\n",
    "    sns.heatmap(adjacency - gt_adjacency, ax=ax1, cbar=False, **kwargs)\n",
    "\n",
    "    ax2.set_title(\"Learned\")\n",
    "    ax1.set_title(\"Learned - GT\")\n",
    "    ax3.set_title(\"Ground truth\")\n",
    "\n",
    "    ax1.set_aspect(\"equal\", adjustable=\"box\")\n",
    "    ax2.set_aspect(\"equal\", adjustable=\"box\")\n",
    "    ax3.set_aspect(\"equal\", adjustable=\"box\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "\n",
    "ROUND_A = torch.where(torch.sigmoid(A) < 1e-1, torch.zeros_like(A), torch.sigmoid(A))\n",
    "plot_adjacency(ROUND_A.cpu().detach().numpy(), A_TRUE.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- NOTEARS\n",
    "- Seb's paper?"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
