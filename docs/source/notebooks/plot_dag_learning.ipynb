{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning a Directed Acyclic Graph (DAG) on data.\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cooper-org/cooper/blob/master/docs/source/notebooks/plot_dag_learning.ipynb)\n",
    "\n",
    "\n",
    "This tutorial considers the problem of learning a Directed Acyclic Graph (DAG) on data. This is a common problem in causal inference, where we are interested in learning the dependency relationships between variables. In this notebook, we will demonstrate how to learn a DAG on data using a {py:class}`~cooper.formulations.QuadraticPenalty` formulation in **Cooper**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# %pip install cooper-optim\n",
    "%pip install --index-url https://test.pypi.org/simple/ --no-deps cooper-optim  # TODO: Remove this line when cooper deployed to pypi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "import cooper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a $d$-dimensional random vector ${X_1, X_2, ..., X_d}$. Given $n$ observations of the random vector $X \\in \\mathbb{R}^{n \\times d}$, we are interested in learning a DAG $G = (V, E)$ whose edges represent the dependencies between the variables. We model the DAG via an adjacency matrix $A \\in \\{0, 1\\}^{d \\times d}$, where $A_{ij} = 1$ if there is an edge from $X_i$ to $X_j$ and $A_{ij} = 0$ otherwise.\n",
    "\n",
    "This problem can be formulated as the following optimization problem:\n",
    "\n",
    "$$\n",
    "\\min_{A \\in \\{0, 1\\}^{d \\times d}} \\left\\| X - XA \\right\\|_F^2 + r \\|A\\|_1,\n",
    "\\quad \\text{s.t.} \\quad A \\text{ is acyclic},\n",
    "$$\n",
    "\n",
    "where $\\| \\cdot \\|_F$ is the Frobenius norm, $r$ is a regularization parameter aimed at encougaing sparsity in the learned DAG, and the constraint ensures that the learned graph is acyclic.\n",
    "\n",
    "{cite:t}`NOTEARS` show that the acyclicity constraint can be formulated as $\\text{tr}(e^{A}) = d$, where $e^{A}$ is the matrix exponential of $A$. This yields the following optimization problem:\n",
    "\n",
    "$$\n",
    "\\min_{A \\in \\{0, 1\\}^{d \\times d}} \\left\\| X - XA \\right\\|_F^2 + r \\|A\\|_1,\n",
    "\\quad \\text{s.t.} \\quad \\text{tr}(e^{A}) = d.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generative process we use is:\n",
    "\n",
    "$X_i \\leftarrow \\sum_{j \\in \\pi_i} X_j + \\epsilon_i$\n",
    "\n",
    "where $\\pi_i$ is the set of parents of $X_i$, and $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ is Gaussian noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "\n",
    "def generate_data(n: int, d: int, n_causes: int, noise_std: float, device: torch.device):\n",
    "    \"\"\"Generate data from a linear structural equation model with Gaussian noise.\n",
    "    The\n",
    "\n",
    "    Args:\n",
    "        n: number of samples\n",
    "        d: number of features\n",
    "        n_causes: number of roots in the graph\n",
    "        noise_std: standard deviation of the noise\n",
    "        device: torch.device\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Data (X) of shape (n, d)\n",
    "        torch.Tensor: Graph (A) of shape (d, d)\n",
    "    \"\"\"\n",
    "    assert n_causes <= d\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Generate the adjacency matrix\n",
    "    # --------------------------------------------\n",
    "\n",
    "    # Rows are nodes, columns are parents\n",
    "    A = torch.zeros(d, d, device=device)\n",
    "\n",
    "    for i in range(n_causes, d):\n",
    "        # For i=1, the only possible parent is 0\n",
    "        parents = 0 if i == 1 else torch.randperm(i)[: np.random.randint(1, i)]\n",
    "\n",
    "        A[i, parents] = 1\n",
    "\n",
    "    assert torch.trace(torch.linalg.matrix_exp(A)).item() == d, \"A is not a DAG\"\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Sample data\n",
    "    # --------------------------------------------\n",
    "\n",
    "    noise = noise_std * torch.randn(n, d, device=device)\n",
    "    X = torch.zeros(n, d, device=device)\n",
    "\n",
    "    for i in range(d):\n",
    "        parents = torch.nonzero(A[i]).flatten()\n",
    "        X[:, i] = X[:, parents].sum(dim=1) + noise[:, i]\n",
    "\n",
    "    # Improve conditioning\n",
    "    X /= math.sqrt(d)\n",
    "\n",
    "    return X, A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate the data and visualize the underlying DAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 5\n",
    "N = 5_000\n",
    "N_CAUSES = 1\n",
    "NOISE_STD = 1e-2\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Generate data\n",
    "X, A_TRUE = generate_data(N, D, N_CAUSES, NOISE_STD, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "G.add_nodes_from(range(D))\n",
    "for i in range(D):\n",
    "    for j in range(D):\n",
    "        if A_TRUE[i, j] != 0:\n",
    "            G.add_edge(j, i)\n",
    "\n",
    "pos = nx.shell_layout(G)\n",
    "\n",
    "plt.figure(figsize=(5, 2))\n",
    "nx.draw(G, pos, with_labels=True, font_weight=\"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the Problem\n",
    "\n",
    "We will use the {py:class}`~cooper.formulations.QuadraticPenalty` formulation to solve the problem. This leads to the following formulation of the constrained optimization problem:\n",
    "\n",
    "$$\n",
    "\\min_{A \\in \\{0, 1\\}^{d \\times d}} \\left\\| X - XA \\right\\|_F^2 + r \\|A\\|_1 + \\frac{c}{2}[\\text{tr}(e^{A}) - d]^2,\n",
    "$$\n",
    "\n",
    "where $c$ is a penalty parameter. We will use a {py:class}`cooper.penalty_coefficients.AdditivePenaltyCoefficientUpdater` to increase the penalty coefficient $c$ whenever the constraint is violated beyond a tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConstrainedMinimizationProblem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAGLearning(cooper.ConstrainedMinimizationProblem):\n",
    "    def __init__(self, X: torch.Tensor, r: float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.X = X\n",
    "        self.n, self.d = X.shape\n",
    "        self.r = r\n",
    "\n",
    "        penalty_coefficient = cooper.penalty_coefficients.DensePenaltyCoefficient(\n",
    "            init=torch.tensor(1.0, device=X.device),\n",
    "        )\n",
    "        self.constraint = cooper.Constraint(\n",
    "            constraint_type=cooper.ConstraintType.EQUALITY,\n",
    "            formulation_type=cooper.formulations.QuadraticPenalty,\n",
    "            penalty_coefficient=penalty_coefficient,\n",
    "        )\n",
    "\n",
    "    def compute_cmp_state(self, A: torch.Tensor) -> cooper.CMPState:\n",
    "        loss = torch.linalg.norm(self.X - self.X @ A.T, ord=\"fro\") ** 2\n",
    "        loss += self.r * torch.linalg.norm(A, ord=1)\n",
    "\n",
    "        constraint_value = torch.trace(torch.linalg.matrix_exp(A)) - self.d\n",
    "        constraint_state = cooper.ConstraintState(violation=constraint_value)\n",
    "\n",
    "        return cooper.CMPState(loss=loss, observed_constraints={self.constraint: constraint_state})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.nn.Parameter(torch.randn(D, D, device=DEVICE) / math.sqrt(D))\n",
    "\n",
    "R = 1e-3\n",
    "PRIMAL_LR = 1e-2\n",
    "MOMENTUM = 0.9\n",
    "N_STEPS = 2_000\n",
    "\n",
    "cmp = DAGLearning(X, R)\n",
    "\n",
    "primal_optimizer = torch.optim.SGD([A], lr=PRIMAL_LR, momentum=MOMENTUM)\n",
    "constrained_optimizer = cooper.optim.UnconstrainedOptimizer(cmp=cmp, primal_optimizers=primal_optimizer)\n",
    "\n",
    "# Increase the penalty coefficient by `increment` if the constraint is violate by more\n",
    "# than `violation_tolerance`\n",
    "penalty_scheduler = cooper.penalty_coefficients.AdditivePenaltyCoefficientUpdater(\n",
    "    increment=1.0,\n",
    "    violation_tolerance=1e-4,\n",
    ")\n",
    "\n",
    "\n",
    "steps, losses, violations, penalty_coefficients = [], [], [], []  # for plotting\n",
    "for i in range(N_STEPS):\n",
    "    roll_out = constrained_optimizer.roll(compute_cmp_state_kwargs={\"A\": A})\n",
    "\n",
    "    A.data.fill_diagonal_(0)  # set the diagonal to zero to prevent self-loops\n",
    "    A.data.clamp_(min=0, max=1)  # ensure that A is a valid adjacency matrix\n",
    "\n",
    "    # Update the penalty coefficient\n",
    "    constraint_state = roll_out.cmp_state.observed_constraints[cmp.constraint]\n",
    "    penalty_scheduler.update_penalty_coefficient_(cmp.constraint, constraint_state)\n",
    "\n",
    "    loss = roll_out.loss.item()\n",
    "    violation = constraint_state.violation.item()\n",
    "    penalty_coefficient_value = cmp.constraint.penalty_coefficient().item()\n",
    "\n",
    "    if i % (N_STEPS // 100) == 0:\n",
    "        steps.append(i)\n",
    "        losses.append(loss)\n",
    "        violations.append(violation)\n",
    "        penalty_coefficients.append(penalty_coefficient_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_adjacency(adjacency, gt_adjacency):\n",
    "    \"\"\"Plot side by side: 1)the learned adjacency matrix, 2)the ground truth adj\n",
    "    matrix and 3)the difference of these matrices\n",
    "    :param np.ndarray adjacency: learned adjacency matrix\n",
    "    :param np.ndarray gt_adjacency: ground truth adjacency matrix\n",
    "    :param str exp_path: path where to save the image\n",
    "    :param str name: additional suffix to add to the image name\n",
    "    \"\"\"\n",
    "    plt.clf()\n",
    "    _, (ax1, ax2, ax3) = plt.subplots(ncols=3, nrows=1)\n",
    "\n",
    "    kwargs = {\"vmin\": 0, \"vmax\": 1, \"cmap\": \"Blues\", \"xticklabels\": False, \"yticklabels\": False}\n",
    "    sns.heatmap(adjacency, ax=ax2, cbar=False, **kwargs)\n",
    "    sns.heatmap(gt_adjacency, ax=ax3, cbar=False, **kwargs)\n",
    "    sns.heatmap(adjacency - gt_adjacency, ax=ax1, cbar=False, **kwargs)\n",
    "\n",
    "    ax1.set_title(\"Difference (Learned, GT)\")\n",
    "    ax2.set_title(\"Learned\")\n",
    "    ax3.set_title(\"Ground truth\")\n",
    "\n",
    "    ax1.set_aspect(\"equal\", adjustable=\"box\")\n",
    "    ax2.set_aspect(\"equal\", adjustable=\"box\")\n",
    "    ax3.set_aspect(\"equal\", adjustable=\"box\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_progress(steps, losses, penalty_coefficients):\n",
    "    _, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    ax2 = ax1.twinx()\n",
    "    ax1.plot(steps, losses, \"tab:blue\")\n",
    "    ax2.plot(steps, penalty_coefficients, \"tab:red\")\n",
    "\n",
    "    ax1.set_yscale(\"log\")\n",
    "\n",
    "    ax1.set_xlabel(\"Steps\", fontsize=12)\n",
    "    ax1.set_ylabel(\"Loss\", color=\"tab:blue\", labelpad=10, fontsize=16)\n",
    "    ax2.set_ylabel(\"Penalty coefficient\", color=\"tab:red\", labelpad=10, fontsize=16)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot shows the loss and penalty coefficient as a function of the number of iterations. We can observe the following:\n",
    "\n",
    "* The loss decreases over time and eventually plateaus, as expected due to the noisy data generation process.\n",
    "* The penalty coefficient increases throughout training to enforce the acyclicity constraint. This increase is more pronounced early on because the initial DAG is not acyclic. Later, as the loss-driven updates to the DAG lead to constraint violations, the penalty coefficient continues to rise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_progress(steps, losses, penalty_coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next plot shows the adjacency matrix of the learned DAG, compared to the ground truth DAG. We can observe that we are able to recover the true DAG structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_adjacency(A.cpu().detach().numpy(), A_TRUE.cpu().detach().numpy())"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
